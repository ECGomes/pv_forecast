{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dfdcc9d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T01:51:57.987725Z",
     "start_time": "2022-08-18T01:51:57.981733Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "import scipy.signal\n",
    "import holidays\n",
    "\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "import seaborn as sb\n",
    "\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed655ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T13:23:46.006515Z",
     "start_time": "2022-07-28T13:23:45.988506Z"
    },
    "code_folding": [
     0
    ]
   },
   "source": [
    "# Aux Functions for Solcast and PV data processing\n",
    "\n",
    "def get_solcast(path):\n",
    "    file = pd.read_csv(path)\n",
    "    file.index = pd.to_datetime(file['PeriodEnd'])\n",
    "    file = file.resample('15T').pad()\n",
    "    \n",
    "    return file\n",
    "\n",
    "\n",
    "def get_pv(path):\n",
    "    file = pd.read_csv(path)\n",
    "    file.index = pd.to_datetime(file['datetime_utc'])\n",
    "    file = file.resample('15T').mean()\n",
    "    \n",
    "    return file\n",
    "\n",
    "\n",
    "def get_solcastPV(df1, df2):\n",
    "    '''\n",
    "    df1: PV dataframe\n",
    "    df2: Solcast dataframe\n",
    "    '''\n",
    "    \n",
    "    # Filter both dataframes for 2019 and 2020\n",
    "    try:\n",
    "        temp_df1 = df1['2019':'2021-04-01']\n",
    "        temp_df2 = df2['2019':'2021-04-01']\n",
    "        \n",
    "        # Check if data is complete. If not, match the smaller indexes\n",
    "        if temp_df2.shape[0] < temp_df1.shape[0]:\n",
    "            last_entry = temp_df2.index\n",
    "            temp_df1 = temp_df1['2019':'{}'.format(temp_df2.index[-1].tz_convert(None))]\n",
    "\n",
    "\n",
    "        # Only considering 2019 and 2020 since data is complete for that period\n",
    "        temp_data = pd.DataFrame({'PV': temp_df1['pv'].values}, index=temp_df1.index)\n",
    "        for i in np.arange(3, len(temp_df2.columns)):\n",
    "            temp_data[temp_df2.columns[i]] = temp_df2[temp_df2.columns[i]].shift(-1).values\n",
    "            \n",
    "        return temp_data\n",
    "    except:\n",
    "        temp_df1 = df1['2019':'2020']\n",
    "        temp_df2 = df2['2019':'2020']\n",
    "        \n",
    "        # Check if data is complete. If not, match the smaller indexes\n",
    "        if temp_df2.shape[0] < temp_df1.shape[0]:\n",
    "            last_entry = temp_df2.index\n",
    "            temp_df1 = temp_df1['2019':'{}'.format(temp_df2.index[-1].tz_convert(None))]\n",
    "\n",
    "\n",
    "        # Only considering 2019 and 2020 since data is complete for that period\n",
    "        temp_data = pd.DataFrame({'PV': temp_df1['pv'].values}, index=temp_df1.index)\n",
    "        for i in np.arange(3, len(temp_df2.columns)):\n",
    "            temp_data[temp_df2.columns[i]] = temp_df2[temp_df2.columns[i]].shift(-1).values\n",
    "            \n",
    "        return temp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44c6154",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T16:10:26.317642Z",
     "start_time": "2022-07-28T16:10:17.824550Z"
    },
    "code_folding": [
     0
    ]
   },
   "source": [
    "# Get data and build a dictionary for preprocessing\n",
    "\n",
    "data = {}\n",
    "\n",
    "folders = glob.glob('C:/Users/FEEL/Jupyter/ecgomes/upacs_study/data/*')\n",
    "for folder in folders:\n",
    "    # Load each of the files inside the folder\n",
    "    temp_pv = get_pv('{}/pv.csv'.format(folder))\n",
    "    temp_solcast = get_solcast('{}/solcast.csv'.format(folder))\n",
    "    \n",
    "    # Join the files into a single dataframe\n",
    "    temp_upac = get_solcastPV(temp_pv, temp_solcast)\n",
    "    \n",
    "    temp_name = folder.split('\\\\')[1]\n",
    "    data[temp_name] = temp_upac\n",
    "    \n",
    "    print('{} date range: {} - {}'.format(temp_name, temp_upac.index[0], temp_upac.index[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bfeedd1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:27:53.765663Z",
     "start_time": "2022-08-16T17:27:53.703137Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Get the data from the UK dataset\n",
    "\n",
    "data = {}\n",
    "\n",
    "temp_data = pd.read_csv('UK/train/pv_train_set4.csv')\n",
    "temp_data.index = pd.to_datetime(temp_data['datetime'])\n",
    "temp_data.drop('datetime', axis=1, inplace=True)\n",
    "temp_data = temp_data.resample('15T').pad()\n",
    "\n",
    "data['uk'] = temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82561f5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:27:54.202889Z",
     "start_time": "2022-08-16T17:27:54.185886Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Aux Functions for adding 2D time information\n",
    "\n",
    "import datetime\n",
    "\n",
    "def days_2d(df):\n",
    "    '''\n",
    "    Adds 2D time information for single days\n",
    "    df: dataframe to add the information\n",
    "    '''\n",
    "    # Map the index into seconds\n",
    "    timestamp_s = pd.to_datetime(df.index.values).map(datetime.datetime.timestamp)\n",
    "    \n",
    "    # Since we're calculating the cos and sin values from seconds, it's 60 seconds into 60 min into 24 hours per day\n",
    "    day_calc = 24*60*60\n",
    "    \n",
    "    # Calculate the values\n",
    "    dayx = np.cos((2*np.pi/day_calc) * timestamp_s)\n",
    "    dayy = np.sin((2*np.pi/day_calc) * timestamp_s)\n",
    "    \n",
    "    return dayx, dayy\n",
    "    \n",
    "\n",
    "def years_2d(df):\n",
    "    '''\n",
    "    Adds 2D time representation throught a year\n",
    "    df: dataframe to add the information\n",
    "    '''\n",
    "    # Add Year Information\n",
    "\n",
    "    day_year = df.index.dayofyear\n",
    "    year_constant = 365.2524\n",
    "\n",
    "    yearx = np.cos((2*np.pi/year_constant) * day_year)\n",
    "    yeary = np.sin((2*np.pi/year_constant) * day_year)\n",
    "    \n",
    "    return yearx, yeary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74610334",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T17:27:54.971712Z",
     "start_time": "2022-08-16T17:27:54.651392Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Add the 2D time information to the data\n",
    "\n",
    "for upac in data.keys():\n",
    "    dayx, dayy = days_2d(data[upac])\n",
    "    yearx, yeary = years_2d(data[upac])\n",
    "    \n",
    "    data[upac]['Day X'] = dayx\n",
    "    data[upac]['Day Y'] = dayy\n",
    "    \n",
    "    data[upac]['Year X'] = yearx\n",
    "    data[upac]['Year Y'] = yeary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "32b62a0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T01:54:52.798959Z",
     "start_time": "2022-08-18T01:54:52.792966Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FEEL\\AppData\\Local\\Temp/ipykernel_10228/4210865765.py:8: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  data_train[upac] = data[upac]['2018']\n"
     ]
    }
   ],
   "source": [
    "# Split the data for training, validation and testing\n",
    "\n",
    "data_train = {}\n",
    "data_val = {}\n",
    "data_test = {}\n",
    "\n",
    "for upac in data.keys():\n",
    "    data_train[upac] = data[upac]['2018']\n",
    "    data_val[upac] = data[upac]['2019-01':'2019-03']\n",
    "    data_test[upac] = data[upac]['2019-04':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5d83cb71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T01:54:54.242486Z",
     "start_time": "2022-08-18T01:54:54.231813Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Aux Function for filtering data\n",
    "\n",
    "def filter_by_points(df, frequency='D', num_points=1440, return_dictionary=False):\n",
    "    \n",
    "    df_dropped = df.dropna()\n",
    "    grouper = df_dropped.groupby(pd.Grouper(freq=frequency))\n",
    "    \n",
    "    output = 0\n",
    "    if return_dictionary:\n",
    "        new_dict = {}\n",
    "        for i in grouper:\n",
    "            if (len(i[1]) != num_points):\n",
    "                pass\n",
    "            else:\n",
    "                new_dict[i[0]] = pd.DataFrame(i[1])\n",
    "        output = new_dict\n",
    "    else:\n",
    "        new_df = pd.DataFrame({})\n",
    "        for i in grouper:\n",
    "            if (len(i[1]) != num_points):\n",
    "                pass\n",
    "            else:\n",
    "                new_df = new_df.append(pd.DataFrame(i[1]))\n",
    "        output = new_df\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e88e48d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T01:54:55.247491Z",
     "start_time": "2022-08-18T01:54:54.873401Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Filter the data by number of points that should be present in a single day\n",
    "\n",
    "filtered_train = {}\n",
    "filtered_val = {}\n",
    "filtered_test = {}\n",
    "\n",
    "for upac in data_train.keys():\n",
    "    filtered_train[upac] = filter_by_points(data_train[upac], frequency='D', num_points=1440/15)\n",
    "    filtered_val[upac] = filter_by_points(data_val[upac], frequency='D', num_points=1440/15)\n",
    "    filtered_test[upac] = filter_by_points(data_test[upac], frequency='D', num_points=1440/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aa039a0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T01:54:55.499717Z",
     "start_time": "2022-08-18T01:54:55.495716Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Select columns to use\n",
    "\n",
    "USED_COLUMNS = ['irradiance_Wm-2', \n",
    "                'pv_power_mw', \n",
    "                'panel_temp_C', \n",
    "                'Day Y', 'Day X',\n",
    "                'Year Y', 'Year X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "526ab2d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T20:50:43.685338Z",
     "start_time": "2022-08-18T20:50:43.670335Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Data Normalization\n",
    "# We don't want to normalize PV so we can capture diferences more easily\n",
    "\n",
    "# Feature range\n",
    "# PV - greater than 0\n",
    "# Irradiance - normalize between 0 and 1\n",
    "# Panel Temp - Unchanged\n",
    "# Day X, Y and Year X and Y - already between -1 and 1\n",
    "\n",
    "MAX_GHI = 3.810000 # max value on the training set\n",
    "\n",
    "normalized_train = {}\n",
    "normalized_val = {}\n",
    "normalized_test = {}\n",
    "\n",
    "for upac in filtered_train.keys():\n",
    "    normalized_train[upac] = filtered_train[upac][USED_COLUMNS].copy(deep=True)\n",
    "    normalized_val[upac] = filtered_val[upac][USED_COLUMNS].copy(deep=True)\n",
    "    normalized_test[upac] = filtered_test[upac][USED_COLUMNS].copy(deep=True)\n",
    "    \n",
    "    normalized_train[upac]['irradiance_Wm-2'] = normalized_train[upac]['irradiance_Wm-2'] / MAX_GHI\n",
    "    normalized_val[upac]['irradiance_Wm-2'] = normalized_val[upac]['irradiance_Wm-2'] / MAX_GHI\n",
    "    normalized_test[upac]['irradiance_Wm-2'] = normalized_test[upac]['irradiance_Wm-2'] / MAX_GHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "704a243f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T20:50:49.540650Z",
     "start_time": "2022-08-18T20:50:49.526647Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Split the data into X and y\n",
    "\n",
    "X_train = {}\n",
    "y_train = {}\n",
    "\n",
    "X_val = {}\n",
    "y_val = {}\n",
    "\n",
    "X_test = {}\n",
    "y_test = {}\n",
    "\n",
    "for upac in normalized_train.keys():\n",
    "    trainx = normalized_train[upac].drop('pv_power_mw', axis=1)\n",
    "    trainy = normalized_train[upac]['pv_power_mw']\n",
    "    valx = normalized_val[upac].drop('pv_power_mw', axis=1)\n",
    "    valy = normalized_val[upac]['pv_power_mw']\n",
    "    testx = normalized_test[upac].drop('pv_power_mw', axis=1)\n",
    "    testy = normalized_test[upac]['pv_power_mw']\n",
    "    \n",
    "    X_train[upac] = trainx\n",
    "    X_val[upac] = valx\n",
    "    X_test[upac] = testx\n",
    "    \n",
    "    y_train[upac] = trainy\n",
    "    y_val[upac] = valy\n",
    "    y_test[upac] = testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0cf6f415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T20:50:50.744920Z",
     "start_time": "2022-08-18T20:50:50.741920Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import XGBoost\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7d387bc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T20:50:54.717811Z",
     "start_time": "2022-08-18T20:50:54.710809Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a training loop for UPACs\n",
    "\n",
    "from joblib import dump, load\n",
    "import optuna\n",
    "\n",
    "def train_upac(upac_name, trainx, trainy, valx, valy, testx, testy, ntrials=100, nruns=10):\n",
    "    # First do a parameter sweep with Optuna\n",
    "    def create_model(trial):\n",
    "        # Do search for n_estimators, max_depth, reg_alpha and reg_lambda\n",
    "        sug_estimators = trial.suggest_int('n_estimators', 50, 5000)\n",
    "        sug_depth = trial.suggest_int('max_depth', 10, 5000)\n",
    "        sug_alpha = trial.suggest_float('reg_alpha', 1e-5, 1e-3)\n",
    "        sug_lambda = trial.suggest_float('reg_lambda', 1e-5, 1e-3)\n",
    "\n",
    "        sug_model = xgb.XGBRegressor(n_estimators=sug_estimators,\n",
    "                                     max_depth=sug_depth,\n",
    "                                     reg_alpha=sug_alpha,\n",
    "                                     reg_lambda=sug_lambda)\n",
    "\n",
    "        return sug_model\n",
    "\n",
    "\n",
    "    def create_training(model):\n",
    "        model.fit(trainx[upac_name], trainy[upac_name])\n",
    "    \n",
    "    \n",
    "    def create_evaluation(model):\n",
    "        temp_yhat = model.predict(valx[upac_name])\n",
    "        return sklearn.metrics.mean_squared_error(valy[upac_name], temp_yhat)\n",
    "    \n",
    "    \n",
    "    def create_objective(trial):\n",
    "        # Instantiate the model\n",
    "        temp_model = create_model(trial)\n",
    "\n",
    "        # Train the model\n",
    "        create_training(temp_model)\n",
    "\n",
    "        # Evaluate model\n",
    "        metrics_val = create_evaluation(temp_model)\n",
    "\n",
    "        return metrics_val\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(create_objective, n_trials=ntrials, show_progress_bar=True)\n",
    "    \n",
    "    IPython.display.clear_output()\n",
    "    \n",
    "    \n",
    "    # Then train different models using the best parameters found\n",
    "    model_dictionary = {}\n",
    "    for i in np.arange(nruns):\n",
    "        temp_model = xgb.XGBRegressor(n_estimators=study.best_params['n_estimators'],\n",
    "                                      max_depth=study.best_params['max_depth'],\n",
    "                                      reg_alpha=study.best_params['reg_alpha'],\n",
    "                                      reg_lambda=study.best_params['reg_lambda'])\n",
    "        \n",
    "        # Train the model\n",
    "        temp_model.fit(trainx[upac_name],#['Ghi'].values.reshape(trainx[upac_name].values.shape[0], 1),\n",
    "                       trainy[upac_name])\n",
    "        \n",
    "        # Save -> dump(example_model, 'example_model.joblib')\n",
    "        dump(temp_model, 'models/xgboost/{}_all/Model {:02d}.joblib'.format(upac_name, i+1))\n",
    "        \n",
    "        # Add it to the dictionary to return\n",
    "        model_dictionary['Model {:02d}'.format(i+1)] = temp_model\n",
    "        \n",
    "    return study, model_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "df04342f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T20:50:56.840287Z",
     "start_time": "2022-08-18T20:50:56.822283Z"
    }
   },
   "outputs": [],
   "source": [
    "# Aux Function for predicting and storing values\n",
    "\n",
    "def do_predictions(dictionary, save_path, X, y, index):\n",
    "    # Go through each model in the dictionary\n",
    "    for model in dictionary.keys():\n",
    "        print('Doing {}'.format(model))\n",
    "        \n",
    "        temp_path = '{}/{}.csv'.format(save_path, model)\n",
    "        \n",
    "        y_pred = dictionary[model].predict(X)\n",
    "        y_pred = pd.DataFrame(y_pred, columns=['pv_power_mw'],\n",
    "                              index=index)\n",
    "        \n",
    "        y_pred.to_csv(temp_path)\n",
    "        \n",
    "    # Also save ground-truth data at the end of the loop\n",
    "    y_true = pd.DataFrame(y, columns=['pv_power_mw'],\n",
    "                          index=index)\n",
    "    \n",
    "    temp_path_gt = '{}/gt.csv'.format(save_path)\n",
    "    y_true.to_csv(temp_path_gt)\n",
    "    \n",
    "    \n",
    "def predict_upacs(model_dictionary, upac_name, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    # Simply call the function above for each of the settings to simplify\n",
    "    \n",
    "    print('Doing training for {}'.format(upac_name))\n",
    "    temp_path = 'results/xgboost/{}_all/train'.format(upac_name)\n",
    "    do_predictions(dictionary=model_dictionary, \n",
    "                   save_path=temp_path, \n",
    "                   X=X_train[upac_name],#[['CloudOpacity', 'GtiFixedTilt', 'Day Y', 'Year X']], \n",
    "                   y=y_train[upac_name],\n",
    "                   index=normalized_train[upac_name].index)\n",
    "    IPython.display.clear_output()\n",
    "    \n",
    "    print('Doing validation for {}'.format(upac_name))\n",
    "    temp_path = 'results/xgboost/{}_all/val'.format(upac_name)\n",
    "    do_predictions(dictionary=model_dictionary, \n",
    "                   save_path=temp_path, \n",
    "                   X=X_val[upac_name],#[['CloudOpacity', 'GtiFixedTilt', 'Day Y', 'Year X']], \n",
    "                   y=y_val[upac_name],\n",
    "                   index=normalized_val[upac_name].index)\n",
    "    IPython.display.clear_output()\n",
    "    \n",
    "    print('Doing testing for {}'.format(upac_name))\n",
    "    temp_path = 'results/xgboost/{}_all/test'.format(upac_name)\n",
    "    do_predictions(dictionary=model_dictionary, \n",
    "                   save_path=temp_path, \n",
    "                   X=X_test[upac_name],#[['CloudOpacity', 'GtiFixedTilt', 'Day Y', 'Year X']], \n",
    "                   y=y_test[upac_name],\n",
    "                   index=normalized_test[upac_name].index)\n",
    "    IPython.display.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d69fdc33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T20:57:54.627933Z",
     "start_time": "2022-08-18T20:50:59.046782Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train UPAC08 - all features\n",
    "\n",
    "uk_study, uk_models = train_upac(upac_name='uk', \n",
    "                                 trainx=X_train, \n",
    "                                 trainy=y_train,\n",
    "                                 valx=X_val, valy=y_val,\n",
    "                                 testx=X_test, testy=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "001ed764",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T20:58:00.604272Z",
     "start_time": "2022-08-18T20:58:00.590270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 2760,\n",
       " 'max_depth': 16,\n",
       " 'reg_alpha': 0.0009559348595186199,\n",
       " 'reg_lambda': 0.00035892687695778615}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UPAC08 best params\n",
    "\n",
    "uk_study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "99e960cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T20:58:08.399020Z",
     "start_time": "2022-08-18T20:58:05.133288Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict UPAC08 - All features\n",
    "predict_upacs(model_dictionary=uk_models, \n",
    "              upac_name='uk',\n",
    "              X_train=X_train, y_train=y_train,\n",
    "              X_val=X_val, y_val=y_val,\n",
    "              X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2531cdd6",
   "metadata": {},
   "source": [
    "########################################### Ghi ################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ea58b31b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T14:16:17.392699Z",
     "start_time": "2022-08-18T14:16:17.372704Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a training loop for UPACs\n",
    "\n",
    "from joblib import dump, load\n",
    "import optuna\n",
    "\n",
    "def train_upac_ghi(upac_name, trainx, trainy, valx, valy, testx, testy, ntrials=100, nruns=10):\n",
    "    # First do a parameter sweep with Optuna\n",
    "    def create_model(trial):\n",
    "        # Do search for n_estimators, max_depth, reg_alpha and reg_lambda\n",
    "        sug_estimators = trial.suggest_int('n_estimators', 50, 5000)\n",
    "        sug_depth = trial.suggest_int('max_depth', 10, 5000)\n",
    "        sug_alpha = trial.suggest_float('reg_alpha', 1e-5, 1e-3)\n",
    "        sug_lambda = trial.suggest_float('reg_lambda', 1e-5, 1e-3)\n",
    "\n",
    "        sug_model = xgb.XGBRegressor(n_estimators=sug_estimators,\n",
    "                                     max_depth=sug_depth,\n",
    "                                     reg_alpha=sug_alpha,\n",
    "                                     reg_lambda=sug_lambda)\n",
    "\n",
    "        return sug_model\n",
    "\n",
    "\n",
    "    def create_training(model):\n",
    "        model.fit(trainx[upac_name], trainy[upac_name])\n",
    "    \n",
    "    \n",
    "    def create_evaluation(model):\n",
    "        temp_yhat = model.predict(valx[upac_name])\n",
    "        return sklearn.metrics.mean_squared_error(valy[upac_name], temp_yhat)\n",
    "    \n",
    "    \n",
    "    def create_objective(trial):\n",
    "        # Instantiate the model\n",
    "        temp_model = create_model(trial)\n",
    "\n",
    "        # Train the model\n",
    "        create_training(temp_model)\n",
    "\n",
    "        # Evaluate model\n",
    "        metrics_val = create_evaluation(temp_model)\n",
    "\n",
    "        return metrics_val\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(create_objective, n_trials=ntrials, show_progress_bar=True)\n",
    "    \n",
    "    IPython.display.clear_output()\n",
    "    \n",
    "    \n",
    "    # Then train different models using the best parameters found\n",
    "    model_dictionary = {}\n",
    "    for i in np.arange(nruns):\n",
    "        temp_model = xgb.XGBRegressor(n_estimators=study.best_params['n_estimators'],\n",
    "                                      max_depth=study.best_params['max_depth'],\n",
    "                                      reg_alpha=study.best_params['reg_alpha'],\n",
    "                                      reg_lambda=study.best_params['reg_lambda'])\n",
    "        \n",
    "        # Train the model\n",
    "        temp_model.fit(trainx[upac_name]['irradiance_Wm-2'].values.reshape(trainx[upac_name].values.shape[0], 1),\n",
    "                       trainy[upac_name])\n",
    "        \n",
    "        # Save -> dump(example_model, 'example_model.joblib')\n",
    "        dump(temp_model, 'models/xgboost/{}_ghi/Model {:02d}.joblib'.format(upac_name, i+1))\n",
    "        \n",
    "        # Add it to the dictionary to return\n",
    "        model_dictionary['Model {:02d}'.format(i+1)] = temp_model\n",
    "        \n",
    "    return study, model_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "25480204",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T14:49:47.398231Z",
     "start_time": "2022-08-18T14:49:47.383236Z"
    }
   },
   "outputs": [],
   "source": [
    "# Aux Function for predicting and storing values\n",
    "\n",
    "def do_predictions(dictionary, save_path, X, y, index):\n",
    "    # Go through each model in the dictionary\n",
    "    for model in dictionary.keys():\n",
    "        print('Doing {}'.format(model))\n",
    "        \n",
    "        temp_path = '{}/{}.csv'.format(save_path, model)\n",
    "        \n",
    "        y_pred = dictionary[model].predict(X)\n",
    "        y_pred = pd.DataFrame(y_pred, columns=['pv_power_mw'],\n",
    "                              index=index)\n",
    "        \n",
    "        y_pred.to_csv(temp_path)\n",
    "        \n",
    "    # Also save ground-truth data at the end of the loop\n",
    "    y_true = pd.DataFrame(y, columns=['pv_power_mw'],\n",
    "                          index=index)\n",
    "    \n",
    "    temp_path_gt = '{}/gt.csv'.format(save_path)\n",
    "    y_true.to_csv(temp_path_gt)\n",
    "    \n",
    "    \n",
    "def predict_upacs_ghi(model_dictionary, upac_name, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    # Simply call the function above for each of the settings to simplify\n",
    "    \n",
    "    print('Doing training for {}'.format(upac_name))\n",
    "    temp_path = 'results/xgboost/{}_ghi/train'.format(upac_name)\n",
    "    do_predictions(dictionary=model_dictionary, \n",
    "                   save_path=temp_path, \n",
    "                   X=X_train[upac_name]['irradiance_Wm-2'].values.reshape(X_train[upac_name].values.shape[0], 1),\n",
    "                   y=y_train[upac_name],\n",
    "                   index=normalized_train[upac_name].index)\n",
    "    IPython.display.clear_output()\n",
    "    \n",
    "    print('Doing validation for {}'.format(upac_name))\n",
    "    temp_path = 'results/xgboost/{}_ghi/val'.format(upac_name)\n",
    "    do_predictions(dictionary=model_dictionary, \n",
    "                   save_path=temp_path, \n",
    "                   X=X_val[upac_name]['irradiance_Wm-2'].values.reshape(X_val[upac_name].values.shape[0], 1),\n",
    "                   y=y_val[upac_name],\n",
    "                   index=normalized_val[upac_name].index)\n",
    "    IPython.display.clear_output()\n",
    "    \n",
    "    print('Doing testing for {}'.format(upac_name))\n",
    "    temp_path = 'results/xgboost/{}_ghi/test'.format(upac_name)\n",
    "    do_predictions(dictionary=model_dictionary, \n",
    "                   save_path=temp_path, \n",
    "                   X=X_test[upac_name]['irradiance_Wm-2'].values.reshape(X_test[upac_name].values.shape[0], 1),\n",
    "                   y=y_test[upac_name],\n",
    "                   index=normalized_test[upac_name].index)\n",
    "    IPython.display.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cc2eacfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T14:47:26.545154Z",
     "start_time": "2022-08-18T14:39:57.718712Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train UK - Ghi\n",
    "\n",
    "uk_study_ghi, uk_models_ghi = train_upac_ghi(upac_name='uk', \n",
    "                                             trainx=X_train, \n",
    "                                             trainy=y_train,\n",
    "                                             valx=X_val, valy=y_val,\n",
    "                                             testx=X_test, testy=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6b6bc6a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T14:49:03.336113Z",
     "start_time": "2022-08-18T14:49:03.320118Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 2008,\n",
       " 'max_depth': 12,\n",
       " 'reg_alpha': 0.0009480380052388284,\n",
       " 'reg_lambda': 0.00029855095072569226}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ghi best params\n",
    "\n",
    "uk_study_ghi.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "730ff419",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T14:49:54.167867Z",
     "start_time": "2022-08-18T14:49:50.311997Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict UK - Ghi\n",
    "predict_upacs_ghi(model_dictionary=uk_models_ghi, \n",
    "                  upac_name='uk',\n",
    "                  X_train=X_train, y_train=y_train,\n",
    "                  X_val=X_val, y_val=y_val,\n",
    "                  X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bfead0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
